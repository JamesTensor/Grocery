{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户两次购买的时间间隔，最近一次购买离现在的时间间隔\n",
    "#看一下相关系数热力图\n",
    "#进行特征筛选\n",
    "#挨个去掉导致过拟合的特征\n",
    "#加树深\n",
    "#看看有偏特征要不要转正态\n",
    "#通常把一些连续值特征、值空间不大的categorical特征都丢给GBDT模型；空间很大的ID特征（比如商品ID）留在LR模型中训练，既能做高阶特征组合又能利用线性模型易于处理大规模稀疏数据的优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The Dask Engine for Modin is experimental.\n",
      "Using TensorFlow backend.\n",
      "FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import modin.pandas as mpd  \n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "import random\n",
    "import scipy.special as special\n",
    "from scipy.sparse import hstack\n",
    "import time\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing.data import OneHotEncoder\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#年龄处理\n",
    "time_now = int(time.time())\n",
    "time_local = time.localtime(time_now)\n",
    "now_year = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_local)\n",
    "now_year = int(now_year[:4])\n",
    "def age_process(birthday):\n",
    "    #len(list(user_df.loc[2,'birthday'])) = 12,后三位为毫秒\n",
    "    try:\n",
    "        time_local = time.localtime(int(birthday[:9]))\n",
    "    except:\n",
    "        return 4\n",
    "    birth_year = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_local)\n",
    "    age = now_year - int(birth_year[:4])\n",
    "    return age\n",
    "\n",
    "def register_type_fun(row):\n",
    "    register_type_list = row.split(',')\n",
    "    tmp_list = [0,0,0,0,0,0]\n",
    "    if 'weixin' in register_type_list:\n",
    "        tmp_list[0] = 1\n",
    "    if 'sns' in register_type_list:\n",
    "        tmp_list[1] = 1\n",
    "    if 'sina_weibo' in register_type_list:\n",
    "        tmp_list[2] = 1\n",
    "    if 'qq_zone' in register_type_list:\n",
    "        tmp_list[3] = 1\n",
    "    if 'missing' in register_type_list:\n",
    "        tmp_list[5] = 1\n",
    "    if 'cellphone' in register_type_list:\n",
    "        tmp_list[4] = 1\n",
    "    if 'xiaomi' in register_type_list:\n",
    "        tmp_list[5] = 1\n",
    "    if 'device' in register_type_list:\n",
    "        tmp_list[5] = 1\n",
    "    return tmp_list\n",
    "\n",
    "def price_process(price):\n",
    "    price = int(price)\n",
    "    if price==0:\n",
    "        return 'price_0'\n",
    "    elif price<20:\n",
    "        return 'price_0_20'\n",
    "    elif price<40:\n",
    "        return 'price_20_40'\n",
    "    elif price<60:\n",
    "        return 'price_40_60'\n",
    "    elif price<80:\n",
    "        return 'price_60_80'\n",
    "    elif price<100:\n",
    "        return 'price_80_100'\n",
    "    elif price<120:\n",
    "        return 'price_100_120'\n",
    "    elif price<140:\n",
    "        return 'price_120_140'\n",
    "    elif price<160:\n",
    "        return 'price_140_160'\n",
    "    elif price<180:\n",
    "        return 'price_160_180'\n",
    "    elif price<200:\n",
    "        return 'price_180_200'\n",
    "    elif price<220:\n",
    "        return 'price_200_220'\n",
    "    elif price<240:\n",
    "        return 'price_220_240'\n",
    "    elif price<260:\n",
    "        return 'price_240_260'\n",
    "    elif price<280:\n",
    "        return 'price_260_280'\n",
    "    elif price<300:\n",
    "        return 'price_280_300'\n",
    "    elif price<350:\n",
    "        return 'price_300_350'\n",
    "    elif price<400:\n",
    "        return 'price_350_400'\n",
    "    elif price<500:\n",
    "        return 'price_400_500'\n",
    "    elif price>=500:\n",
    "        return 'price_lager_than_500'\n",
    "    else:\n",
    "        return 'price_none'\n",
    "\n",
    "def columbus_user_tags_fun(row):\n",
    "    columbus_user_tags_list = row.split(',')\n",
    "    tmp_list = [0,0,0,0,0,0]\n",
    "    if 'weixin' in register_type_list:\n",
    "        tmp_list[0] = 1\n",
    "    if 'sns' in register_type_list:\n",
    "        tmp_list[1] = 1\n",
    "    if 'sina_weibo' in register_type_list:\n",
    "        tmp_list[2] = 1\n",
    "    if 'qq_zone' in register_type_list:\n",
    "        tmp_list[3] = 1\n",
    "    if 'missing' in register_type_list:\n",
    "        tmp_list[5] = 1\n",
    "    if 'cellphone' in register_type_list:\n",
    "        tmp_list[4] = 1\n",
    "    if 'xiaomi' in register_type_list:\n",
    "        tmp_list[5] = 1\n",
    "    if 'device' in register_type_list:\n",
    "        tmp_list[5] = 1\n",
    "    return tmp_list\n",
    "\n",
    "def time_process(data_time):\n",
    "    try:\n",
    "        time_tmp = time.localtime(int(data_time[:9]))\n",
    "    except:\n",
    "        return 4\n",
    "    time_tmp = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_tmp)\n",
    "    year = list(time_tmp)[:4]\n",
    "    month = list(time_tmp)[5:7]\n",
    "    day = list(time_tmp)[9:11]\n",
    "    hour = list(time_tmp)[3]\n",
    "    return year,month,day,hour \n",
    "\n",
    "def activate_channel_label_fun(slug):\n",
    "    slug = str(slug)\n",
    "    if slug=='record_eating_activity':\n",
    "        return 1\n",
    "    elif slug=='record_weight':\n",
    "        return 2\n",
    "    elif slug=='post_create':\n",
    "        return 3\n",
    "    elif slug=='habit_checkin':\n",
    "        return 4\n",
    "    elif slug=='search_food':\n",
    "        return 5\n",
    "    elif slug=='finished_sports':\n",
    "        return 6\n",
    "    else:  \n",
    "        return 0\n",
    "    \n",
    "def special_condition_label_fun(x):\n",
    "    x = str(x)\n",
    "    if x=='0.0' or x=='0':\n",
    "        return 0\n",
    "    elif x=='1.0' or x=='1':\n",
    "        return 1\n",
    "    elif x=='2.0' or x=='2':\n",
    "        return 2\n",
    "    elif x=='3.0' or x=='3':\n",
    "        return 3\n",
    "    else:  \n",
    "        return 4\n",
    "\n",
    "# 四分位箱线图处理异常值\n",
    "def box_plot(data_series,feature_name):\n",
    "    data_series = pd.Series(sorted(data_series))\n",
    "    #q_abnormal_low = data_series.quantile(0.25) - 1.5 * (data_series.quantile(0.75) - data_series.quantile(0.25))\n",
    "    q_abnormal_up = data_series.quantile(0.90) + 1.5 * (data_series.quantile(0.90) - data_series.quantile(0.25))\n",
    "    q_abnormal_up = int(round(q_abnormal_up,0))\n",
    "    data_series = data_series.apply(lambda x:q_abnormal_up if x>q_abnormal_up else x)\n",
    "    return data_series\n",
    "\n",
    "def _load_data(data_file):\n",
    "    tmp_df = mpd.read_csv(filepath_or_buffer = data_file,sep=\",\",names=user_feature_list)\n",
    "    #tmp_df = mpd.DataFrame._to_pandas(tmp_df)\n",
    "    return tmp_df\n",
    "\n",
    "def app_version_func(app_version):\n",
    "    if app_version == '7.4' or app_version =='9.4':\n",
    "        app_version = 'high'\n",
    "    elif app_version == '7.3' or app_version =='9.3':\n",
    "        app_version = 'medium'\n",
    "    elif app_version == 'nan':\n",
    "         app_version == 'missing'\n",
    "    else:\n",
    "        app_version = 'low'\n",
    "    return app_version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时间函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_year(date_timestamp): \n",
    "    time_tmp = time.localtime(int(date_timestamp/1000))\n",
    "    date = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_tmp)\n",
    "    year = int(date[:4]) \n",
    "    \n",
    "    if year <= 2017:\n",
    "        year=2017\n",
    "    return year\n",
    "\n",
    "def timestamp_to_month(date_timestamp): \n",
    "    time_tmp = time.localtime(int(date_timestamp/1000))\n",
    "    date = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_tmp) \n",
    "    month = int(date[5:7])\n",
    "    return month\n",
    "\n",
    "def timestamp_to_day(date_timestamp): \n",
    "    time_tmp = time.localtime(int(date_timestamp/1000))\n",
    "    date = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_tmp)\n",
    "    day = int(date[8:11])\n",
    "    return day\n",
    "\n",
    "def timestamp_to_hour(date_timestamp): \n",
    "    time_tmp = time.localtime(int(date_timestamp/1000))\n",
    "    date = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_tmp)\n",
    "    hour = int(date[11:13])\n",
    "    if 12 < hour <= 7:\n",
    "        hour = 'hour_night'\n",
    "    elif 8 < hour <= 11:\n",
    "        hour = 'hour_moring'\n",
    "    elif 11 < hour <= 13:\n",
    "        hour = 'hour_noon'\n",
    "    elif 13 < hour <= 17:\n",
    "        hour = 'hour_afternoon'\n",
    "    elif 18 < hour <= 12:\n",
    "       hour = 'hour_evening'\n",
    "    else:\n",
    "        hour = 'hour_missing'\n",
    "    return hour\n",
    "\n",
    "def timestamp_to_week(date_timestamp): \n",
    "    date_datetime = datetime.fromtimestamp(int(date_timestamp/1000)) \n",
    "    week = date_datetime.strftime('%A')\n",
    "    if week == 'Sunday' or week == 'Saturday':\n",
    "        week == 'weekend'\n",
    "    elif week == 'Tuesday':\n",
    "        week == 'tuesday'\n",
    "    elif week == 'Monday' or week == 'Wednesday' or week == 'Thursday' or week == 'Friday':\n",
    "        week == 'weekday'\n",
    "    else:\n",
    "        week ==\"week_missing\"\n",
    "    return week\n",
    "\n",
    "def timestamp_to_season(date_timestamp): \n",
    "    time_tmp = time.localtime(int(date_timestamp/1000))\n",
    "    date = time.strftime(\"%Y-%m-%d %H:%M:%S\",time_tmp) \n",
    "    month = int(date[5:7])\n",
    "    if month == 3 or month == 4 or month == 5:\n",
    "        season = 'season_spring'\n",
    "    elif month == 6 or month == 7 or month == 8:\n",
    "        season = 'season_summer'\n",
    "    elif month == 9 or month == 10 or month == 11:\n",
    "        season = 'season_autumn'\n",
    "    elif month == 12 or month == 1 or month == 2:\n",
    "        season = 'season_winter'\n",
    "    else:\n",
    "        season = 'season_missing'\n",
    "    return season\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTR贝叶斯平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CTR贝叶斯平滑\n",
    "class BayesianSmoothing(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "     \n",
    "    #按beta分布生成点击率，再生成点击样本\n",
    "    def sample(self, alpha, beta, num, imp_upperbound):\n",
    "        '''\n",
    "        生成样本数据\n",
    "        :param alpha:alpha分布参数\n",
    "        :param beta:beta分布参数\n",
    "        :param num:样本数量\n",
    "        :param imp_upperbound:展示上限\n",
    "        :return:I展示数量，C点击数量\n",
    "        '''\n",
    "        sample = np.random.beta(alpha, beta, num)\n",
    "        I = []\n",
    "        C = []\n",
    "        for clk_rt in sample:\n",
    "            imp = random.random() * imp_upperbound\n",
    "            clk = imp * clk_rt\n",
    "            I.append(imp)\n",
    "            C.append(clk)\n",
    "        return I, C\n",
    " \n",
    "    def update(self, imps, clks, iter_num, epsilon):\n",
    "        '''\n",
    "        更新beta分布参数，alpha,beta\n",
    "        :param imps: 展示数量\n",
    "        :param clks: 点击数量\n",
    "        :param iter_num: 迭代次数\n",
    "        :param epsilon:终止条件，常数，小于这个常数则不需要更新了\n",
    "        :return:\n",
    "        '''\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "        return self.alpha,self.beta\n",
    " \n",
    "    #通过点击和展示来修正alpha & beta\n",
    "    def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "        '''\n",
    "        :param imps: 展示数量\n",
    "        :param clks: 点击数量\n",
    "        :param alpha:init alpha\n",
    "        :param beta:init beta\n",
    "        :return: 返回新的alpha beta\n",
    "        '''\n",
    "        numerator_alpha = 0.0\n",
    "        numerator_beta = 0.0\n",
    "        denominator = 0.0\n",
    " \n",
    "        for i in range(len(imps)):\n",
    "            numerator_alpha += (special.digamma(clks[i]+alpha) - special.digamma(alpha))\n",
    "            numerator_beta += (special.digamma(imps[i]-clks[i]+beta) - special.digamma(beta))\n",
    "            denominator += (special.digamma(imps[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    " \n",
    "        return alpha*(numerator_alpha/denominator), beta*(numerator_beta/denominator)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #初始点击率r \n",
    "    #bs = BayesianSmoothing(1, 1)\n",
    "    #I, C = bs.sample(500, 500, 1000, 10000)\n",
    "    #bs.update(I, C, 1000, 0.0000000001)\n",
    "    #ctr = []\n",
    "    #for i in range(len(I)):\n",
    "        #ctr.append((C[i]+bs.alpha)/(I[i]+bs.alpha+bs.beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slug转化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slug_to_recommendation_pool(user_df):\n",
    "    user_df.loc[user_df['slug']=='p_ef_dzdqjrb_80g',\"slug\"] = 's_ef_dzdqjrb_80g'\n",
    "    user_df.loc[user_df['slug']=='ef_dzdqjrb_15_20191206',\"slug\"] = 's_ef_dzdqjrb_80g'\n",
    "    user_df.loc[user_df['slug']=='s_easyfun_yingzuidou_180g',\"slug\"] = 's_easyfun_yingzuidou_180g'\n",
    "    user_df.loc[user_df['slug']=='p_ef_dzhjjrc_300g',\"slug\"] = 's_ef_dzhjjrc_300g'\n",
    "    user_df.loc[user_df['slug']=='p_ef_jrc_yw_hj_2dai',\"slug\"] = 's_ef_dzhjjrc_300g'\n",
    "    user_df.loc[user_df['slug']=='p_ef_jrc_xl_hj_2dai',\"slug\"] = 's_ef_dzhjjrc_300g'\n",
    "    user_df.loc[user_df['slug']=='p_ef_dzhjjrc_600g',\"slug\"] = 's_ef_dzhjjrc_300g'\n",
    "    #本来就在推荐池内的\n",
    "    recommend_pool = pd.read_excel('/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/good_feature_data/recommend_pool.xlsx')\n",
    "    recommend_pool_list = recommend_pool['slug'].tolist()\n",
    "    user_pool_df = user_df[user_df['slug'].isin(recommend_pool_list)]\n",
    "    user_pool_df = user_pool_df.rename(columns={'slug':'s_slug'})\n",
    "    #在推荐池中的P开头的SKU，下一步匹配游有用\n",
    "    goods_match_on_sale_df = pd.read_excel('./good_feature_data/goods_match_on_sale_s.xls')\n",
    "    goods_match_on_sale_df = goods_match_on_sale_df[['s_slug','p_slug','goods_slug']]\n",
    "    goods_match_on_sale_df[['p_slug','goods_slug']] = goods_match_on_sale_df[['p_slug','goods_slug']].applymap((lambda x: \"\".join(x.split()) if type(x) is str else x))\n",
    "    goods_match_on_sale_df['goods_slug'] = goods_match_on_sale_df['goods_slug'].astype(str)\n",
    "    goods_match_on_sale_df['p_slug'] = goods_match_on_sale_df['p_slug'].astype(str)\n",
    "    goods_match_on_sale_df['s_slug'] = goods_match_on_sale_df['s_slug'].astype(str)\n",
    "    user_df['slug'] = user_df['slug'].astype(str)\n",
    "    p_list = ['p_ea10days_1512g','p_easyace_ysjnew_xmy_190828','p_ef_funsichi_nwlb','p_ef_zkxsd_757g','p_ea_3daypttp'\n",
    "    ,'p_7394376616792','p_7394376616815','p_easyfun_madai_tea_20']\n",
    "    pp_user_df = user_df[user_df['slug'].isin(p_list)]\n",
    "    #pp_not_buy_match_df = pp_not_buy_match_df.rename(columns={'slug': 's_slug'})\n",
    "    #pp_user_df = pp_user_df.drop_duplicates('slug')\n",
    "    pp_user_df = pp_user_df.groupby(by=['slug'])['c'].sum().to_frame()\n",
    "    pp_user_df['slug'] =pp_user_df.index\n",
    "    pp_user_df =pp_user_df.reset_index(drop= True)\n",
    "    #匹配\n",
    "    pp_user_df = pd.merge(pp_user_df,goods_match_on_sale_df,right_on='p_slug',left_on='slug',how='inner')\n",
    "    goods_user_match_df = pd.merge(user_df,goods_match_on_sale_df,right_on='goods_slug',left_on='slug',how='inner')\n",
    "    #找到匹配成功的\n",
    "    goods_user_match_df = goods_user_match_df[~goods_user_match_df['goods_slug'].isnull()]\n",
    "    pp_user_df = pp_user_df[~pp_user_df['p_slug'].isnull()]\n",
    "    #goods_user_match_df = goods_user_match_df.groupby(by=['s_slug'])['c'].sum()\n",
    "    #goods_user_match_df = goods_user_match_df.drop_duplicates('s_slug')\n",
    "    #pp_user_df = pp_user_df.groupby(by=['s_slug'])['c'].sum()\n",
    "    user_match_df =pd.concat([goods_user_match_df,pp_user_df],sort=False)\n",
    "    user_match_df = user_match_df.groupby(by=['s_slug'])['c'].sum().to_frame()\n",
    "    user_match_df['s_slug'] =user_match_df.index\n",
    "    user_match_df =user_match_df.reset_index(drop= True)\n",
    "    user_match_pool_df = user_match_df[user_match_df['s_slug'].isin(recommend_pool_list)]\n",
    "    user_all_df = pd.concat([user_match_pool_df,user_pool_df],sort=False)\n",
    "    user_all_df = user_all_df.groupby(by=['s_slug'])['c'].sum().to_frame()\n",
    "    user_all_df['s_slug'] =user_all_df.index\n",
    "    user_all_df =user_all_df.reset_index(drop= True)\n",
    "    user_all_df = user_all_df[['s_slug','c']]\n",
    "    return user_all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data_file_list = [\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_12.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_11.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_10.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_9.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_8.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_7.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_6.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_5.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_4.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_3.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_2.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_1.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_0.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-12.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-11.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-10.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-9.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-8.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-7.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-6.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-5.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-4.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-3.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-2.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_-1.csv',\n",
    "]\n",
    "positive_data_file_list = [\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/1.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/2.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/3.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/4.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/5.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/6.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/7.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/8.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/9.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/10.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/11.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/12.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/13.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/14.csv', \n",
    "]\n",
    "negative_data_file_list = [\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_5.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_4.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_3.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_2.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_1.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_0.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_10.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_9.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_8.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_7.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_6.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_12.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0102/用户反例原始数据/view_comment_not_add_buy_user_11.csv',\n",
    "]\n",
    "positive_data_file_list = [\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/1.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/2.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/3.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/4.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/5.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/6.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/7.csv',\n",
    "'/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0202/8.csv',\n",
    "]\n",
    "\n",
    "user_feature_list = [\n",
    " 'second_id','slug','time','birthday','register_timestamp','first_visit_time','survey_finished_at',\n",
    " 'first_traffic_source_type','register_source_app_key','register_type',\n",
    " 'register_sns','gender','phone_model','app_channel','app_version','os_version','province','city_level','purpose',\n",
    " 'target_weight','start_weight','latest_weight','bmi','budget_calory',\n",
    " 'view_goods_count','add_cart_count','commit_order_count','num_paid','eka_num_paid',\n",
    " 'total_cost','recent_buy_at','record_food_count','search_food_count','record_weight_count',\n",
    " 'finished_sports_count','habbit_checkin_count','post_create_count',\n",
    " 'active_day_count','activate_channel','special_condition','easy_customer_first_buy_time',\n",
    " 'easy_membership_first_platform','easy_membership_first_time','user_membership_status','user_fans_status',\n",
    " 'guide_to_bh_at','wx_assistant_friend','columbus_user_tags'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in positive_data_file_list:\n",
    "    positive_user_tmp_df =  _load_data(i)\n",
    "    try:\n",
    "        positive_user_df = mpd.concat([positive_user_df,positive_user_tmp_df],ignore_index=True)\n",
    "        print('1')\n",
    "    except:\n",
    "        positive_user_df = positive_user_tmp_df\n",
    "        print('2')\n",
    "positive_user_df = mpd.DataFrame._to_pandas(positive_user_df)\n",
    "positive_user_df['label'] = 1\n",
    "for i in negative_data_file_list:\n",
    "    negative_user_tmp_df =  _load_data(i)\n",
    "    try:\n",
    "        negative_user_df = mpd.concat([negative_user_df,negative_user_tmp_df],ignore_index=True)\n",
    "        print('3')\n",
    "    except:\n",
    "        negative_user_df = negative_user_tmp_df\n",
    "        print('4')\n",
    "negative_user_df = mpd.DataFrame._to_pandas(negative_user_df)\n",
    "negative_user_df['label'] = 0\n",
    "user_df = pd.concat([negative_user_df,positive_user_df],ignore_index=True)\n",
    "user_df = user_df[~user_df['second_id'].isin(['second_id'])]\n",
    "user_df = user_df[~user_df['second_id'].isin(['distinct_id'])]\n",
    "user_df['second_id'] = user_df['second_id'].astype(int)\n",
    "user_df['slug'] = user_df['slug'].astype(str)\n",
    "user_df.drop_duplicates(subset=['second_id', 'slug'], keep='first',inplace=True)\n",
    "del user_df['columbus_user_tags']\n",
    "del user_df['os_version']\n",
    "del negative_user_df\n",
    "del positive_user_df\n",
    "del negative_user_tmp_df\n",
    "del positive_user_tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phonemodel特征,不要重复跑，不然报错\n",
    "#user_df['phone_model'] = user_df['phone_model'].fillna('1')\n",
    "user_df.loc[(user_df[\"phone_model\"]=='iPhone'),\"phone_model\"] = 2\n",
    "user_df.loc[(user_df[\"phone_model\"]!='iPhone'),\"phone_model\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#purpose\n",
    "user_df.loc[(user_df[\"purpose\"]=='lost_weight'),\"purpose_label\"] = 1\n",
    "user_df.loc[(user_df[\"purpose\"]=='keep_muscle'),\"purpose_label\"] = 2\n",
    "user_df.loc[(user_df[\"purpose\"]=='gain_muscle'),\"purpose_label\"] = 3\n",
    "user_df.loc[(user_df[\"purpose\"]!='lost_weight') & (user_df[\"purpose\"]!='keep_muscle') &(user_df[\"purpose\"]!='gain_muscle'),\"purpose_label\"] = 4\n",
    "user_df['purpose_label'] = user_df['purpose_label'].fillna(5)\n",
    "del user_df['purpose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#个人信息特征空值，不要重复跑，不然报错\n",
    "user_df['city_level'].fillna(2,inplace=True)\n",
    "user_df['city_level'] = user_df['city_level'].astype(np.int32)\n",
    "#province_list = user_df['province'].value_counts().keys().tolist()[:37]\n",
    "province_list=['广东省', '江苏省', '浙江省', '北京', '上海', '山东省', '河南省', '四川省', '湖北省', '河北省', '湖南省', '安徽省', '辽宁省', '福建省', '陕西省', '重庆', '江西省', '天津', '云南省', '黑龙江省', '山西省', '未知', '广西', '吉林省', '内蒙古', '贵州省', '新疆', '甘肃省', '海南省', '宁夏', '香港', '青海省', '台湾省', '澳门', '西藏']\n",
    "user_df['province'].fillna('未知',inplace = True)\n",
    "user_df['province'] = user_df['province'].apply(lambda x:x if x in province_list else '国外')\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(user_df['province'])\n",
    "user_df['province'] = encoder.transform(user_df['province'])\n",
    "user_df[\"gender\"].fillna(2, inplace=True)\n",
    "user_df['gender'] = user_df['gender'].apply(lambda x:1 if x=='1' or x==1 or x==1.0 else 2)\n",
    "user_df[\"activate_channel\"] = user_df[\"activate_channel\"].apply(activate_channel_label_fun)\n",
    "user_df[\"special_condition\"] = user_df[\"special_condition\"].apply(special_condition_label_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户行为特征\n",
    "feature_box_plot_list = ['view_goods_count', 'record_food_count', 'search_food_count','record_weight_count','active_day_count']                              \n",
    "for i in feature_box_plot_list:\n",
    "    user_df[i].fillna(0,inplace=True)\n",
    "    user_df[i] = user_df[i].astype(np.int32)\n",
    "    user_df[i] = box_plot(user_df[i],i)\n",
    "    user_df[i].fillna(0,inplace=True)\n",
    "    \n",
    "# PND模型处理异常值\n",
    "feature_custom_list = ['habbit_checkin_count','total_cost','num_paid','add_cart_count','eka_num_paid','commit_order_count','post_create_count','finished_sports_count']\n",
    "for i in feature_custom_list:\n",
    "    user_df[i].fillna(0,inplace=True)\n",
    "    user_df[i] = user_df[i].astype(np.float32)\n",
    "user_df['habbit_checkin_count'] = user_df['habbit_checkin_count'].apply(lambda x:200 if x>200 else x)\n",
    "user_df['add_cart_count'] = user_df['add_cart_count'].apply(lambda x:80 if x>80 else x)\n",
    "user_df['total_cost'] = user_df['total_cost'].apply(lambda x:5000 if x>5000 else x)\n",
    "user_df['num_paid'] = user_df['num_paid'].apply(lambda x:20 if x>20 else x)\n",
    "user_df['eka_num_paid'] = user_df['eka_num_paid'].apply(lambda x:20 if x>20 else x)\n",
    "user_df['commit_order_count'] = user_df['commit_order_count'].apply(lambda x:30 if x>30 else x)\n",
    "user_df['post_create_count'] = user_df['post_create_count'].apply(lambda x:50 if x>50 else x)\n",
    "user_df['finished_sports_count'] = user_df['finished_sports_count'].apply(lambda x:80 if x>80 else x)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户体制特征使用平均填充\n",
    "feature_body_list = [\"target_weight\",\"start_weight\",\"latest_weight\",\"bmi\",\"budget_calory\"]\n",
    "for i in feature_body_list:\n",
    "    user_df[i] = user_df[i].astype(np.float32)\n",
    "    mean_val = user_df[i].mean()\n",
    "    user_df[i].fillna(mean_val, inplace=True)\n",
    "del mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#年龄特征处理:确定年龄阈值，写入列表，按列表分段后encode\n",
    "user_df['birthday'] = user_df['birthday'].astype(str)\n",
    "user_df['age'] = user_df['birthday'].apply(age_process)\n",
    "age_cut_list = [0,16,18, 20, 22, 24, 26,28,30,32,34,36,38,45,60,200]\n",
    "user_df['age_label'] = pd.cut(user_df['age'], age_cut_list)\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['age_label'])\n",
    "user_df['age_label'] = labelencoder.transform(user_df['age_label'])\n",
    "del user_df['age']\n",
    "del user_df['birthday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['register_sns'] = user_df['register_sns'].fillna('missing')\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['register_sns'])\n",
    "user_df['register_sns_label'] = labelencoder.transform(user_df['register_sns'])\n",
    "del user_df['register_sns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#app_channel onehot\n",
    "app_channel_list = ['missing', 'huawei','nearme', 'vivo','xiaomi','boohee','tencent','meizu', '_360','baidu','wandoujia','undefind','lenovo','zhihuan','oppo','other',]\n",
    "user_df['app_channel'] = user_df['app_channel'].apply(lambda x:'vivo' if x == 'oppo' else x)\n",
    "user_df['app_channel'] = user_df['app_channel'].apply(lambda x:x if x in app_channel_list else 'other')\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['app_channel'])\n",
    "user_df['app_channel_label'] = labelencoder.transform(user_df['app_channel'])\n",
    "del user_df['app_channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注册方式onehot\n",
    "user_df['register_type'] = user_df['register_type'].fillna('missing')\n",
    "user_df['register_type']= pd.DataFrame(user_df['register_type'].apply(register_type_fun))\n",
    "user_df['register_type_weixin'] = user_df['register_type'].apply(lambda x:x[0])\n",
    "user_df['register_type_sns'] = user_df['register_type'].apply(lambda x:x[1])\n",
    "user_df['register_type_sina_weibo'] = user_df['register_type'].apply(lambda x:x[2])\n",
    "user_df['register_type_qq_zone'] = user_df['register_type'].apply(lambda x:x[3])\n",
    "user_df['register_type_cellphone'] = user_df['register_type'].apply(lambda x:x[4])\n",
    "user_df['register_type_missing'] = user_df['register_type'].apply(lambda x:x[5])\n",
    "del user_df['register_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['first_traffic_source_type'].fillna('missing',inplace=True)\n",
    "user_df['first_traffic_source_type'] = user_df['first_traffic_source_type'].apply(lambda x:'direct' if x == '直接流量' else x)\n",
    "user_df['first_traffic_source_type'] = user_df['first_traffic_source_type'].apply(lambda x:'indirect' if x == '引荐流量'  else x)\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['first_traffic_source_type'])\n",
    "user_df['first_traffic_source_type_label'] = labelencoder.transform(user_df['first_traffic_source_type'])\n",
    "del user_df['first_traffic_source_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['register_source_app_key'] = user_df['register_source_app_key'].apply(lambda x:x if x == 'ifood' or x == 'boohee_weixin' or x == 'one' or x == 'food' or x == 'nice' else 'missing')            \n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['register_source_app_key'])\n",
    "user_df['register_source_app_key_label'] = labelencoder.transform(user_df['register_source_app_key'])\n",
    "del user_df['register_source_app_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['easy_membership_first_platform'].fillna('missing',inplace=True)\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['easy_membership_first_platform'])\n",
    "user_df['easy_membership_first_platform_label'] = labelencoder.transform(user_df['easy_membership_first_platform'])\n",
    "del user_df['easy_membership_first_platform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['user_membership_status'].fillna('missing',inplace=True)\n",
    "user_df['user_membership_status'] = user_df['user_membership_status'].apply(lambda x:'member_expire' if x == 'member_canceled' else x)\n",
    "user_df['user_membership_status'] = user_df['user_membership_status'].apply(lambda x:'member_expire' if x == 'member_inactive' else x)\n",
    "user_df['user_membership_status'] = user_df['user_membership_status'].apply(lambda x:'member_ongoing' if x == 'associate_member' else x)\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['user_membership_status'])\n",
    "user_df['user_membership_status_label'] = labelencoder.transform(user_df['user_membership_status'])\n",
    "del user_df['user_membership_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['user_fans_status'].fillna('missing',inplace=True)\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['user_fans_status'])\n",
    "user_df['user_fans_status_label'] = labelencoder.transform(user_df['user_fans_status'])\n",
    "del user_df['user_fans_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['wx_assistant_friend'].fillna('missing',inplace=True)\n",
    "user_df['wx_assistant_friend'] = user_df['wx_assistant_friend'].apply(lambda x:'yes' if x == '是' else x)\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['wx_assistant_friend'])\n",
    "user_df['wx_assistant_friend_label'] = labelencoder.transform(user_df['wx_assistant_friend'])\n",
    "del user_df['wx_assistant_friend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#时间戳特征处理\n",
    "timestamp_feature_list = ['register_timestamp','first_visit_time','survey_finished_at',\n",
    " 'recent_buy_at','easy_customer_first_buy_time','easy_membership_first_time','guide_to_bh_at']\n",
    "for i in timestamp_feature_list:\n",
    "    user_null_df = user_df[user_df[i].isnull() == True]\n",
    "    user_df = user_df[user_df[i].isnull() == False]\n",
    "    user_df[i] = user_df[i].astype(int)\n",
    "    user_df[i+'_year'] = user_df[i].apply(timestamp_to_year)\n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    labelencoder.fit(user_df[i+'_year'])\n",
    "    user_df[i+'_year_label'] = labelencoder.transform(user_df[i+'_year'])\n",
    "    del user_df[i+'_year']\n",
    "    user_df[i+'_season'] = user_df[i].apply(timestamp_to_season)\n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    labelencoder.fit(user_df[i+'_season'])\n",
    "    user_df[i+'_season_label'] = labelencoder.transform(user_df[i+'_season'])\n",
    "    del user_df[i+'_season']\n",
    "    user_df[i+'_month'] = user_df[i].apply(timestamp_to_month)\n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    labelencoder.fit(user_df[i+'_month'])\n",
    "    user_df[i+'_month_label'] = labelencoder.transform(user_df[i+'_month'])\n",
    "    del user_df[i+'_month']\n",
    "    user_df[i+'_week'] = user_df[i].apply(timestamp_to_week)\n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    labelencoder.fit(user_df[i+'_week'])\n",
    "    user_df[i+'_week_label'] = labelencoder.transform(user_df[i+'_week'])\n",
    "    del user_df[i+'_week']\n",
    "    #user_df[i+'_day'] = user_df[i].apply(timestamp_to_day)\n",
    "    #user_df= user_df.join(pd.get_dummies(user_df[i+'_day'],prefix=i+'_day_'))\n",
    "    #del user_df[i+'_day']\n",
    "    user_df[i+'_hour'] = user_df[i].apply(timestamp_to_hour)\n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    labelencoder.fit(user_df[i+'_hour'])\n",
    "    user_df[i+'_hour_label'] = labelencoder.transform(user_df[i+'_hour'])   \n",
    "    del user_df[i+'_hour']\n",
    "    del user_df[i]\n",
    "    del user_null_df[i]\n",
    "    user_df = pd.concat([user_df,user_null_df],sort=False)\n",
    "user_df = user_df.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#app版本特征\n",
    "user_df['app_version'] = user_df['app_version'].fillna('nan')\n",
    "user_df['app_version'] = user_df['app_version'].astype(str)\n",
    "user_df['app_version'] = user_df['app_version'].apply(lambda x:x[:3])\n",
    "user_df['app_version'] = user_df['app_version'].apply(app_version_func)\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(user_df['app_version'])\n",
    "user_df['app_version_label'] = labelencoder.transform(user_df['app_version']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slug维度的点击、加购、购买比例，转化率，排名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 样本sku处理成推荐池内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_df.loc[user_df['slug']=='p_ef_dzdqjrb_80g',\"slug\"] = 's_ef_dzdqjrb_80g'\n",
    "#user_df.loc[user_df['slug']=='ef_dzdqjrb_15_20191206',\"slug\"] = 's_ef_dzdqjrb_80g'\n",
    "#user_df.loc[user_df['slug']=='s_easyfun_yingzuidou_180g',\"slug\"] = 's_easyfun_yingzuidou_180g'\n",
    "#user_df.loc[user_df['slug']=='p_ef_dzhjjrc_300g',\"slug\"] = 's_ef_dzhjjrc_300g'\n",
    "#user_df.loc[user_df['slug']=='p_ef_jrc_yw_hj_2dai',\"slug\"] = 's_ef_dzhjjrc_300g'\n",
    "#user_df.loc[user_df['slug']=='p_ef_jrc_xl_hj_2dai',\"slug\"] = 's_ef_dzhjjrc_300g'\n",
    "#user_df.loc[user_df['slug']=='p_ef_dzhjjrc_600g',\"slug\"] = 's_ef_dzhjjrc_300g'\n",
    "#s_ef_qyzdrwh_112g_slug_list = ['s_ef_qyzdrwh_112g','p_ef_qyzdrwh_112g','p_ef_qyzdrwh_112g_1','s_coffee_190819_1','p_ef_weihua_jianguocui_20200103','p_ef_weihua_kekejgc_20200103','ef_kkdzc_160g','ef_qyzdrwh_112g','ef_qmjgc_new_160g']   \n",
    "\n",
    "mapping_dic = {\n",
    "    's_ef_qyzdrwh_112g':'s_ef_qyzdrwh_112g',\n",
    "    'p_ef_qyzdrwh_112g':'s_ef_qyzdrwh_112g',\n",
    "    'p_ef_qyzdrwh_112g_1':'s_ef_qyzdrwh_112g',\n",
    "    's_coffee_190819_1':'s_ef_qyzdrwh_112g',\n",
    "    'p_ef_weihua_jianguocui_20200103':'s_ef_qyzdrwh_112g',\n",
    "    'p_ef_weihua_kekejgc_20200103':'s_ef_qyzdrwh_112g',\n",
    "    'ef_kkdzc_160g':'s_ef_qyzdrwh_112g',\n",
    "    'ef_qyzdrwh_112g':'s_ef_qyzdrwh_112g',\n",
    "    'ef_qmjgc_new_160g':'s_ef_qyzdrwh_112g',\n",
    "    'p_ef_dzhjjrc_600g':'s_ef_dzhjjrc_300g',\n",
    "    'p_ef_jrc_xl_hj_2dai':'s_ef_dzhjjrc_300g',\n",
    "    'p_ef_jrc_yw_hj_2dai':'s_ef_dzhjjrc_300g',\n",
    "    'p_ef_dzhjjrc_300g':'s_ef_dzhjjrc_300g',\n",
    "    'p_easyfun_yingzuidou_180g':'s_easyfun_yingzuidou_180g',\n",
    "    'ef_dzdqjrb_15_20191206':\"s_ef_dzdqjrb_80g\",\n",
    "    'p_ef_dzdqjrb_80g':'s_ef_dzdqjrb_80g',\n",
    "  \n",
    "}\n",
    "for i,v in mapping_dic.items():\n",
    "    user_df.loc[user_df['slug']==i,\"slug\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#本来就在推荐池内的\n",
    "recommend_pool = pd.read_excel('/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/good_feature_data/recommend_pool.xlsx')\n",
    "recommend_pool_list = recommend_pool['slug'].tolist()\n",
    "user_pool_df = user_df[user_df['slug'].isin(recommend_pool_list)]\n",
    "user_pool_df = user_pool_df.rename(columns={'slug':'s_slug'})\n",
    "user_pool_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在推荐池中的P开头的SKU，下一步匹配游有用\n",
    "goods_match_on_sale_df = pd.read_excel('./good_feature_data/goods_match_on_sale_s.xls')\n",
    "goods_match_on_sale_df = goods_match_on_sale_df[['s_slug','p_slug','goods_slug']]\n",
    "goods_match_on_sale_df[['p_slug','goods_slug']] = goods_match_on_sale_df[['p_slug','goods_slug']].applymap((lambda x: \"\".join(x.split()) if type(x) is str else x))\n",
    "goods_match_on_sale_df['goods_slug'] = goods_match_on_sale_df['goods_slug'].astype(str)\n",
    "goods_match_on_sale_df['p_slug'] = goods_match_on_sale_df['p_slug'].astype(str)\n",
    "goods_match_on_sale_df['s_slug'] = goods_match_on_sale_df['s_slug'].astype(str)\n",
    "user_df['slug'] = user_df['slug'].astype(str)\n",
    "p_list = ['p_ea10days_1512g','p_easyace_ysjnew_xmy_190828','p_ef_funsichi_nwlb','p_ef_zkxsd_757g','p_ea_3daypttp'\n",
    ",'p_7394376616792','p_7394376616815','p_easyfun_madai_tea_20']\n",
    "pp_user_df = user_df[user_df['slug'].isin(p_list)]\n",
    "#pp_not_buy_match_df = pp_not_buy_match_df.rename(columns={'slug': 's_slug'})\n",
    "pp_user_df = pp_user_df.drop_duplicates(['second_id','slug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#匹配\n",
    "pp_user_df = pd.merge(pp_user_df,goods_match_on_sale_df,right_on='p_slug',left_on='slug',how='inner')\n",
    "goods_user_match_df = pd.merge(user_df,goods_match_on_sale_df,right_on='goods_slug',left_on='slug',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到匹配成功的\n",
    "goods_user_match_df = goods_user_match_df[~goods_user_match_df['goods_slug'].isnull()]\n",
    "pp_user_df = pp_user_df[~pp_user_df['p_slug'].isnull()]\n",
    "goods_user_match_df = goods_user_match_df.drop_duplicates(['second_id','s_slug'])\n",
    "pp_user_df = pp_user_df.drop_duplicates(['second_id','s_slug'])\n",
    "user_match_df =pd.concat([goods_user_match_df,pp_user_df])\n",
    "user_match_df = user_match_df.drop_duplicates(['second_id','s_slug'])\n",
    "user_match_pool_df = user_match_df[user_match_df['s_slug'].isin(recommend_pool_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_all_df = pd.concat([user_match_pool_df,user_pool_df],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_all_df['slug']\n",
    "del user_all_df['p_slug']\n",
    "del user_all_df['goods_slug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_all_df['time'] = user_all_df['time'].astype(str)\n",
    "user_all_df['time'] = user_all_df['time'].apply(lambda x:x[:10])\n",
    "user_all_df = user_all_df.drop_duplicates(['second_id','s_slug','time'])\n",
    "del user_all_df['time']\n",
    "user_all_df = user_all_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user 购买均价档次，可于商品档次对应\n",
    "user_all_df['average_paid'] = round(user_all_df['total_cost']/user_all_df['num_paid'],0)\n",
    "user_all_df['average_paid'].fillna(0,inplace=True)\n",
    "user_all_df['average_paid'] = user_all_df['average_paid'].astype('int32')\n",
    "user_all_df['average_paid_level'] = 'average_paid_'+user_all_df['average_paid'].apply(price_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 商品维度的查看、加购、购买数据处理成推荐池内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_goods_count_by_slug_df = pd.read_csv(filepath_or_buffer = '/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0127/buy_goods_count_by_slug.csv',sep=\",\")\n",
    "add_cart_count_by_slug_df = pd.read_csv(filepath_or_buffer = '/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0127/add_cart_count_by_slug.csv',sep=\",\")\n",
    "view_goods_count_by_slug_df = pd.read_csv(filepath_or_buffer = '/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/data/0127/view_goods_count_by_slug.csv',sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_goods_count_by_slug_df = slug_to_recommendation_pool(buy_goods_count_by_slug_df)\n",
    "add_cart_count_by_slug_df = slug_to_recommendation_pool(add_cart_count_by_slug_df)\n",
    "view_goods_count_by_slug_df = slug_to_recommendation_pool(view_goods_count_by_slug_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =buy_goods_count_by_slug_df['c'].sum()/view_goods_count_by_slug_df['c'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查是否有遗漏\n",
    "recommend_pool = pd.read_excel('/Users/wjj/Desktop/jupyter/Boohee_GBDT+LR/good_feature_data/recommend_pool.xlsx')\n",
    "recommend_pool_list = list(recommend_pool['slug'])\n",
    "view_goods_count_by_slug_list = view_goods_count_by_slug_df['s_slug'].to_list()\n",
    "add_cart_count_by_slug_list = add_cart_count_by_slug_df['s_slug'].to_list()\n",
    "buy_goods_count_by_slug_list = buy_goods_count_by_slug_df['s_slug'].to_list()\n",
    "list(set(recommend_pool_list).difference(set(view_goods_count_by_slug_list)))\n",
    "len(set(user_all_df['s_slug'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvr_df = pd.merge(buy_goods_count_by_slug_df,view_goods_count_by_slug_df,how='inner',on='s_slug')\n",
    "#buy_goods_count_rank\n",
    "cvr_df = cvr_df.rename(columns={'c_x':'buy_goods_count','c_y':'view_goods_count'})\n",
    "cvr_df = cvr_df.sort_values(by=\"buy_goods_count\",ascending=False)\n",
    "cvr_df['buy_goods_count_rank'] =1+cvr_df.reset_index(drop=True,inplace=False).index\n",
    "#view_goods_count_rank\n",
    "cvr_df = cvr_df.sort_values(by=\"view_goods_count\",ascending=False)\n",
    "cvr_df['view_goods_count_rank'] =1+cvr_df.reset_index(drop=True,inplace=False).index\n",
    "#cvr_feature & rank\n",
    "cvr_df['cvr'] = round(cvr_df['buy_goods_count']/cvr_df['view_goods_count'],2)\n",
    "cvr_df['cvr'] = round(cvr_df['buy_goods_count']/cvr_df['view_goods_count'],2)\n",
    "cvr_df['cvr'] = cvr_df['cvr'] = cvr_df['cvr'].apply(lambda x: 0.9 if x > 0.9 else x)\n",
    "cvr_df['cvr'] = cvr_df['cvr'] = cvr_df['cvr'].apply(lambda x: 0.001 if x < 0.0001 else x)\n",
    "cvr_df['cvr_alpha'] = 1\n",
    "cvr_df['cvr_beta'] = round(((1/cvr_df['cvr'])-1)*(cvr_df['cvr_alpha']),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户两次购买的时间间隔，最近一次购买离现在的时间间隔\n",
    "#通常把一些连续值特征、值空间不大的categorical特征都丢给GBDT模型；空间很大的ID特征（比如商品ID）留在LR模型中训练，既能做高阶特征组合又能利用线性模型易于处理大规模稀疏数据的优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对CTR进行平滑\n",
    "for index,row in cvr_df.iterrows(): \n",
    "    bs = BayesianSmoothing(row['cvr_alpha'], row['cvr_beta'])\n",
    "    I, C = bs.sample(50, 4000, 100, 10000)\n",
    "    alpha,beta = bs.update(I, C, 10, 0.00000001)\n",
    "    cvr_df.iloc[index,6] = alpha\n",
    "    cvr_df.iloc[index,7] = beta\n",
    "    for i in range(len(I)):\n",
    "        cvr_df.iloc[index,5]=(C[i]+alpha)/(I[i]+alpha+beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvr_df = cvr_df.sort_values(by=\"cvr\",ascending=False)\n",
    "cvr_df['cvr_rank'] =1+cvr_df.reset_index(drop=True,inplace=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_all_df = pd.merge(user_all_df,cvr_df,how='inner',left_on='s_slug',right_on='s_slug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_all_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 草稿本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
