逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，它有非线性的部件但是本质上仍然是个线性模型，因为Sigmoid只对结果做了映射，没有影响实际计算部分。

LR可以处理线性和非线性的问题，只不过不能直接处理非线性问题，要人工特征工程先转化映射过。

代价函数：最大似然估计

不能用均方误差，因为它在逻辑回归中是非凸的，会陷入局部最小。

手推LR损失函数并对它求导：https://www.jianshu.com/p/610859e27f66

### 1 为什么适合离散特征

我们在使用逻辑回归的时候很少会把数据直接丢给 LR 来训练，我们一般会对特征进行离散化处理，这样做的优势大致有以下几点：

1. 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
2. 离散后的特征对异常值更具鲁棒性；
3. LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
4. 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
5. 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化；

总的来说，特征离散化以后起到了加快计算，简化模型和增加泛化能力的作用。

### 2 我们需要明确 Sigmoid 函数到底起了什么作用：

- 线性回归是在实数域范围内进行预测，而分类范围则需要在 [0,1]，逻辑回归减少了预测范围；
- 线性回归在实数域上敏感度一致，而逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。

## 逻辑回归的优缺点

**优点：**

- 实现简单，广泛的应用于工业问题上；
- 分类时计算量非常小，速度很快，存储资源低；
- 便利的观测样本概率分数；
- 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；
- 计算代价不高，易于理解和实现；
- 在实际应用中，当我们能够拿到许多低层次的特征时，高维悉数特征，强调记忆性场景，可以考虑使用逻辑回归来解决我们的问题。 
- 损失函数是凸函数，避免局部最小问题。

**缺点：**

- 当特征空间很大时，逻辑回归的性能不是很好；

- 容易**欠拟合**，一般准确度不太高

- 不能很好地处理大量多类特征或变量；

- 本质上还是一种线性模型。

  与线性回归的区别：

  线性回归用均方误差，而LR如果也用均方误差，那么他的损失函数是非凸的。

- ![LR与线性回归关系](/Users/wjj/Desktop/推荐系统笔记/LR与线性回归关系.png)