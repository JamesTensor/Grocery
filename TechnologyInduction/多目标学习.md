## **多目标排序问题的解决方案**

多目标排序问题的解决方案，大概有以下四种：通过改变样本权重、多模型分数融合、排序学习（Learning To Rank，LTR）、多任务学习（Multi-Task Learning，MTL）。

**2.1 通过样本权重进行多目标优化**

如果主目标是点击率，分享功能是我们希望提高的功能。那么点击和分享都是正样本（分享是点击行为的延续），分享的样本可以设置更高的样本权重。模型训练在计算梯度更新参数时，梯度要乘以权重，对样本权重大的样本给予更大的权重。

因此，样本权重大的样本，如果预测错误就会带来更大的损失。通过这种方法能够在优化某个目标（点击率）的基础上，优化其他目标（分享率）。在实际AB测试的过程中会发现，通过调整样本权重的这样的方法，原始目标A会受到一定的损失以换取新增加目标B的增长，实现初级的多目标优化。

优点:模型简单，通过梯度乘以样本权重设计目标函数，不需要额外架构支持，没有增加算法时间复杂度。

缺点：没有对多目标进行建模，而是将多个目标折算成同一个目标。样本的折算权重需要根据AB测试才能确定。比如认为一次分享算两次点击；在视频中停留了2min等价于3次对视频的点击行为等，这里面的数字需要根据线上评价指最优，测试出来的。

**2.2 多模型分数融合**

多模型分数融合就是每个优化目标都由独立的模型来预测一个分数，最后将这些分数采用加权的方式进行融合。如下图，每个模型都有一个得分，通过模型的权重来设计一个最终的得分，完成多目标优化。

优点:模型简单

缺点：

1. 线上服务需要有额外的时间开销，需要将多个模型预测的结果组合。
2. 不同目标难以量化评估重要性。
3. 样本部分特征稀疏，模型准确率低。
4. 模型融合的超参难以学习。

那么如何更好地确定超参呢？

实际应用中，通过AB测试不断尝试，以线下和线上的评价指标提高为目标,找到一组合适的超参。本质上还是多个目标函数的加权。

**2.3 排序学习（Learning To Rank，LTR）**（没看懂，不看）

多模型分数融合通过计算推荐物的综合得分来排序（point-wise），其目的是为了推荐物品。因此，也可以直接预测物品两两之间相对顺序的问题（pair-wise）来解决多目标学习的问题，常用的算法比如 BPR，还可以预测物品序列之间的得分情况（list-wise） 来解决多目标学习的问题。具体可以才看看相关算法的介绍。



**2.4 多任务学习（Multi-Task Learning，MTL）**

**2.4.1 多任务学习介绍**

在多任务学习算法之前，一般采用简单的样本加权、多模型分数融合方法、直接用排序学习的算法来实现多目标排序。现在常用多任务学习来实现多目标排序。

经典的迁移学习知道源和目标，在原有数据和算法参数的肩膀上，采用新的数据基础上训练出更好的符合相似目标的模型，可以更好更快预测新的目标，而多任务学习的学习器并不知道具体的目标，它接受一个综合目标的相关信息，也就是同时学习多个任务。多任务学习也算迁移学习的一个分支。

多任务学习（MTL ）可以设计的非常灵活主要体现在两个方面：

1. 模型架构方面：在深度学习网络中可以共享embedding特征，或者共享中间层的某些隐藏单元，也可以是模型某一层或者最后一层的结果，并且共享之外的部分各自独立。在模型的设计中，层间关系自由组合搭配。
2. 特征组合方面：多个任务可以采用不同的特征组合，有的特征只属于模型架构的某个部分，有些特征整个模型都可以使用。

可以从任务的相关性、内存和CPU使用情况、最终效果、线上性能等方面综合考虑如何设计MTL的结构。

**2.4.1.1参数的硬共享机制（基于参数的共享，Parameter Based）**

基于参数的共享是多目标学习最常用的方法。在深度学习的网络中，通过共享特征和特征的embedding以及隐藏层的网络架构，在最后一层通过全连接+softmax的方式来区分不同任务，最后做一个线性融合来实现多目标排序。

![img](https://pic4.zhimg.com/80/v2-1602f700f0dfed55240f29d073273eab_1440w.jpg)硬共享参数

不同任务的最终目标不同，共享参数和特征其实会限制目标的差异性，最终模型的效果可能会不太好，如果目标间的相关性比较高，那么这种方式的效果会好一些。

美团“猜你喜欢”深度学习排序模型是将点击率和下单率拆分，网络在最后一个全连接层进行拆分，单独学习对应 Loss 的参数。

![img](https://pic1.zhimg.com/80/v2-af78a6bfe9d6ae3612638b277d3995c0_1440w.jpg)美团“猜你喜欢架构

线上预测时，将Click-output和Pay-output做一个线性融合。

**2.4.1.2参数的软共享机制（基于约束的共享，Regularization Based）**

![img](https://pic2.zhimg.com/80/v2-133f78ddf85b3d777994fbabdec052c5_1440w.jpg)

参数的软共享机制，每个任务都有自己的参数和模型结构，可以选择哪些共享哪些不共享。最后通过正则化的方式，来拉进模型参数之间的距离，例如使用 L2 进行正则化。在线上服务的时候，花费的时间要比硬参数共享机制化的多，因为模型结构更加复杂了。

**2.4.2 为什么多任务学习是有效的**

多任务学习有效的原因是引入了归纳偏置（inductive bias），有两个效果:

1. 互相促进：可以把多任务模型之间的关系看作是互相先验知识，也称归纳迁移（inductive transfer），有了对模型的先验假设，可以更好的提升模型的效果；解决数据稀疏性其实本身也是迁移学习的一个特性，在多任务学习中也同样会体现。
2. 泛化作用：不同的模型学到的表征不同，可能A模型学到的是B模型所没有学好的，B模型也有其自身的优点，而这一点很可能是A学不好的一方面。这样一来模型的健壮就更加的强了，具体很好的泛化能力。