![特征工程](/Users/wjj/Desktop/推荐系统笔记/特征工程.jpeg)

# 1、特征预处理：

### A、特征缩放：

保证所有的特征数值具有相同的数量级，消除特征的不同尺度所造成的偏差。主要手段为标准化和归一化，详见

极客时间: https://time.geekbang.org/column/article/9762

### B、异常点：

​	如果是异常点：判断其是否由于失误造成异常。如果其本身造成，那么判断异常点是否来源于第二套机制，也就是第二套分布？一般在数据量不是很缺的情况下，异常点可以去掉。

​	如何判断是异常点：

​	第一种是聚类，比如我们可以用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。我们可以将其从训练集过滤掉。

​	第二种是异常点检测方法，主要是使用iForest或者one class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。

​	第三种统计上通常用3-sigma方法（一般来说，如果某个特征数据，最大值为maxValue，均值为mean，标准差为std。如果满足maxValue>mean+3*std，那么我们就认为这个特征数据存在离群点）检测离群值。 

​	当然，某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止我们将正常的样本过滤掉了。

### C、缺失值：

1、填补：看是该特征是连续值还是离散值。如果是连续值，取平均值或者中位数来填充，如果是离散值，选择众数来填充缺失值。也可以用K近邻直接填充。

2、删除：如果某个样本缺失值过多，可以直接删除整个样本。

3、上面两种方式：取零值会导致相应权重无法进行更新，收敛速度减慢。而取平均值也略显武断，毕竟不同的特征缺失所表示的含义可能不尽相同。

神经网络也可以通过学习的方式自适应地处理缺失值，而不是人为设置默认值。因此设计了如下的Layer来自适应的学习缺失值的权重:![自学习缺失特征处理](/Users/wjj/Desktop/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0/%E8%87%AA%E5%AD%A6%E4%B9%A0%E7%BC%BA%E5%A4%B1%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86.png)

参考：https://tech.meituan.com/2018/03/29/recommend-dnn.html

### D、可以适当移除一些特征

在模型训练之前移除一些特征有助于增强模型的可解释性，也可以降低计算中的开销。如果两个特征之间的相关性较强，或者说具有共性（collinearity），这时就可以删除掉其中的一个，这正是主成分分析的作用。

什么样的特征不具备区分度呢？这里有两个经验性的标准：一是特征取值的总数与样本数目的比例在 10% 以下，这样的特征在 100 个样本里的取值数目不超过 10 个；二是出现频率最高的特征取值的出现频率应该在出现频率次高的特征取值频率次高的特征取值频率的 20 倍以上，如果有 90 个样本的特征取值为 1，4 个样本的特征取值为 2，其余取值的样本数目都在 4 个以下，这样的特征就可以被删除了。

### E、离散化和连续化

连续特征的离散化：人工分区域，离散化

离散特征的连续化：one-hot

### F、偏度（skewness）

是用于描述概率分布非对称性的一个指标。偏度可能由异常点造成或者其本身就存在。

如果是异常点，那么参见B

##### 如果偏度是数据本身自带的：

可以取对数，使得偏度修正到我们能接受的程度。

### G、处理不平衡数据

　　　　我们做分类算法训练时，如果训练集里的各个类别的样本数量不是大约相同的比例，就需要处理样本不平衡问题。也许你会说，不处理会怎么样呢？如果不处理，那么拟合出来的模型对于训练集中少样本的类别泛化能力会很差。举个例子，我们是一个二分类问题，如果训练集里A类别样本占90%，B类别样本占10%。 而测试集里A类别样本占50%， B类别样本占50%， 如果不考虑类别不平衡问题，训练出来的模型对于类别B的预测准确率会很低，甚至低于50%。

　　　　如何解决这个问题呢？一般是两种方法：权重法或者采样法。

　　　　权重法是比较简单的方法，我们可以对训练集里的每个类别加一个权重class weight。如果该类别的样本数多，那么它的权重就低，反之则权重就高。如果更细致点，我们还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用。

　　　　如果权重法做了以后发现预测效果还不好，可以考虑采样法。（上采样和下采样，见西瓜书，以下两段可以不看）

　　　　采样法常用的也有两种思路，一种是对类别样本数多的样本做子采样, 比如训练集里A类别样本占90%，B类别样本占10%。那么我们可以对A类的样本子采样，直到子采样得到的A类样本数和B类别现有样本一致为止，这样我们就只用子采样得到的A类样本数和B类现有样本一起做训练集拟合模型。第二种思路是对类别样本数少的样本做过采样, 还是上面的例子，我们对B类别的样本做过采样，直到过采样得到的B类别样本数加上B类别原来样本一起和A类样本数一致，最后再去拟合模型。

　　　　上述两种常用的采样法很简单，但是都有个问题，就是采样后改变了训练集的分布，可能导致泛化能力差。所以有的算法就通过其他方法来避免这个问题，比如SMOTE算法通过人工合成的方法来生成少类别的样本。方法也很简单，对于某一个缺少样本的类别，它会随机找出几个该类别的样本，再找出最靠近这些样本的若干个该类别样本，组成一个候选合成集合，然后在这个集合中不停的选择距离较近的两个样本，在这两个样本之间，比如中点，构造一个新的该类别样本。举个例子，比如该类别的候选合成集合有两个样本(x1,y),(x2,y)(x1,y),(x2,y),那么SMOTE采样后，可以得到一个新的训练样本(x1+x22,y)(x1+x22,y),通过这种方法，我们可以得到不改变训练集分布的新样本，让训练集中各个类别的样本数趋于平衡。我们可以用imbalance-learn这个Python库中的SMOTEENN类来做SMOTE采样。



# 2、特征选择

指从全部特征中选取一个特征子集，使构造出来的模型更好。

有两种方式：1、直接删除一些特征。2、把不同特征合成，降维。3、添加一些高级特征。

如何选择特征，从两个方面考虑：

- 是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本没有差异，那我们就可以判断，这个特征对于样本的区别并没有什么用
- 是否相关：与目标相关性高的特征应该优先选择

### 特征选择的方式1：直接删除一些特征

#### 过滤法

方差选择法：先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。

相关系数法：先计算各个特征对目标值的相关系数，选择相关系数大于阈值的特征。

#### 包装法

最常用的包装法是递归消除特征法(recursive feature elimination,以下简称RFE)。递归消除特征法使用一个机器学习模型来进行多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进行下一轮训练。

它在第一轮训练的时候，会选择所有的特征来训练，如果有n个特征，那么其选择出ww中分量的平方值w2iwi2最小的那个序号i对应的特征，将其排除，在第二类的时候，特征数就剩下n-1个了，我们继续用这n-1个特征和输出值来训练SVM，同样的，去掉w2iwi2最小的那个序号i对应的特征。以此类推，直到剩下的特征数满足我们的需求为止。

在sklearn中，可以使用RFE函数来实现上述思路。

### 特征选择的方式2：降维

<https://blog.csdn.net/program_developer/article/details/80632779>

主成分分析（Principal Components Analysis，PCA）作为降维中最经典的方法，一种线性、非监督、全局的降维算法。

PCA算法：通过协方差矩阵的特征值分解能够得到数据的主成分，以二维特征为例，两个特征之间可能存在线性关系（例如运动的时速和秒速度），这样就造成了第二维信息是冗余的。PCA的目标是发现这种特征之间的线性关系，并去除。例如sklearn的decomposition.PCA和spark mllib的from [pyspark.ml.feature](/var/folders/sv/qw0xlhj57gs8rfzjp7wdx95w0000gn/T/abnerworks.Typora/AB7471A2-04A6-4ED2-9D2D-077EB4ED6986/pyspark.ml.feature) import PCA

<https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg>

### 特征选择的方式3：添加高级特征

对原生数据再加工，生成有商业意义的数据特征。

在我们拿到已有的特征后，我们还可以根据需要寻找到更多的高级特征。比如有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个二级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加速度这个四级特征。。。也就是说，高级特征可以一直寻找下去。

寻找高级特征最常用的方法有：

　　　　若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。
　　　　若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。
　　　　若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。
　　　　若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。

　　　　当然，寻找高级特征的方法远不止于此，它需要你根据你的业务和模型需要而得，而不是随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型。个人经验是，聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。











## 综述：

事实上，特征工程是一个迭代过程，我们需要不断的设计特征、选择特征、建立模型、评估模型，然后才能得到最终的model。下面是特征工程的一个迭代过程：



1.头脑风暴式特征：意思就是进你可能的从原始数据中提取特征，暂时不考虑其重要性，对应于特征构建；

2.设计特征：根据你的问题，你可以使用自动地特征提取，或者是手工构造特征，或者两者混合使用；

3.选择特征：使用不同的特征重要性评分和特征选择方法进行特征选择；

4.评估模型：使用你选择的特征进行建模，同时使用未知的数据来评估你的模型精度。





### 特征工程Mark：

https://www.zhihu.com/people/runlu/activities
https://zhuanlan.zhihu.com/p/57198837





























