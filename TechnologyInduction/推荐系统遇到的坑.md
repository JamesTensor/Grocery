## **E&E**

E&E, exploration & exploitation

EE要不要做？肯定要做，你不能让用户只能看到一类新闻，这样久了他的推荐领域范围只会越来越小，自己也觉得没劲，所以一定要做兴趣探索。

但是做，一般都是掉指标的，探索的过程是艰难的，大部分时间用户体验上也是负向的。

那么，

- 牺牲多少ctr来保EE才算是合适的？
- EE的ROI什么时候算是>1的？





## **评价指标难以确定**

按照道理来讲，推荐系统要做的事情其实是“推荐用户希望看到的东西”，但是“用户希望看到的东西”落实到指标上，可就让人头大了。

以新闻推荐为例。你说究竟要得到什么呢？

- 高CTR？那么擦边球的软色情(黄赌毒)以及热门文章就会被选出来
- 高Staytime？那么视频+文章feed流就成为为视频feed流和超长文章feed流

Medium的做法是，优化一个f(CTR, staytime,...)的多指标加权的综合指标，但是据我所知，这个加权的系数，还是一个拍脑门的算法。





## **数据分布的不一致**

​           如果仔细排查，既不存在数据泄漏，也没有出现不一致的问题，离线auc明明就是涨了很多，线上就是下降，而且是离线涨的越多，线上下降越多，还有一种可能就是数据的不一致，也就是数据的“冰山效应”----离线训练用的是有偏的冰山上的数据，而在线上预估的时候，需要预测的是整个冰山的数据，包括大量冰面以下的数据！

​          这种情况其实在推荐系统里非常常见，但是往往非常的隐蔽，一时半会很难发现。我们看下面这张图。左边是我们的Baseline，绿色的表示正样本，红色表示负样本，灰色部分表示线上由于推荐系统的“偏见”（预估分数较低），导致根本没有展现过的数据。

​        离线阶段，我们通过各种优化，新模型的离线评估表现更好了，例如图中第二列，可以发现第4个绿色的正样本和第7个绿色的正样本排到了第3和第6的位置，离线的auc指标涨了。

​       到了真正线上的预估也就是第三列，发现对于这部分离线见过的样本，模型的预估序并未改变。但是新模型给了灰色没有见过的数据更高的预估分数，这部分数据一旦表现不好，很可能造成我们前面说的情况，离线（第二列）评估指标明明涨了不少，在线（第三列）评估指标ctr却下降。

![推荐系统的坑](/Users/wjj/Desktop/推荐系统笔记/推荐系统的坑.jpg)

这种情况也不是必现的，在LR以特征工程为主要迭代的时代很少见。主要的原因是模型的前后迭代差异并不大。新模型对比老模型最主要的特点是新加入了一部分特征，往往模型的打分差异并不大，从图中第二列到第三列，原来那些冰山下的数据也就是旧模型预估分数偏低的部分，在新模型中能够脱颖而出拿到很高的预估分数的概率并不高。

​    而在模型有较大变化的时候，例如lr->树模型，lr->深度模型，不同网络结构的深度模型变化，这种情况容易出现，原因就是新旧模型的变化较大，预估分数变化也较大。

举一个简单的例子，假设我们的baseline是A模型，样本都是A模型生产出的热门样本，这个时候我们迭代了B模型，B模型是拟合的A模型作用下样本分布的，到了真正的线上预估的时候，这个时候B模型的上线改变了样本分布，模型自然很难预估好。没有足够好的样本，模型也很难学到足够有用的信息。但是这样会产生新样本来重新训练B模型，慢慢会纠正过来。



## ctr模型提升模型效果的一些经验？

以ctr模型为例，embedding参数对自己业务数据效果是否敏感、网络层数的深度对效果的影响、不同正则化方法是否work、bn/ln/dropout等方法是否有效、当前的特征体系对于高阶特征交叉是否已经刻画足够、attention到底收益有多大等等。这些如果能从更本质和通用共性的角度去分析总结，大概率在同样的数据分布上可以作为以后的先验判断，经验是可迁移的，不至于在每一次有新的模型出来就去盲目的做尝试。

