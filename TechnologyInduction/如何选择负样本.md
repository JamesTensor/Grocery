https://zhuanlan.zhihu.com/p/165064102

### 排序：

曝光未点击作为负样本。



### 召回：

真实场景的召回环节，SSB（sample select bias）问题还是非常突出

只用曝光未点击作为负样本的策略不合适。

原因：**离线训练数据的分布，应该与线上实际应用的数据，保持一致**。

应该用曝光未点击+随机负样本作为总的负样本集。

随机负样本。其中easy negative:hard negative=100:1。

为什么要用hard negative： 增加模型训练的难度，以更大程度区分真的正样本和类似正样本的负样本。例如：Airbnb的召回层中，如果easy negative负样本就是一个上海的user和全国各地的hotel，那模型很简单就能学习到，只有上海的hotel和上海的user点击率会比较高。如果我们增加负样本多采样一些上海的hotel就增加了难度。此为hard negative。

easy negative就是真正的随机负样本，hard negative又两种策略：

1、根据业务：例如：Airbnb增加与正样本同城的房间作为负样本，增强了正负样本在地域上的相似性，加大了模型的学习难度。

2、业务没有明显区分特点就用算法：用上一版本的召回模型筛选出"没那么相似"的<user,item>对，作为额外负样本，训练下一版本召回模型。拿召回位置在101~500上的物料。排名太靠前那是正样本，不能用；太靠后，与随机无异，也不能用；只能取中段。在上一版召回模型中，这批样本只是“相似度”比较靠后而已；而在训练新模型时，直接划为负样本，从“人民内部矛盾”升级为"敌我矛盾"，能够迫使模型进一步与这部分hard negative“划清界限”。这是百度和Facebook两家团队实践过的方案。



不过需要特别强调的是，**hard negative并非要替代easy negative，而是easy negative的补充。在数量上，负样本还是以easy negative为主，文章中经验是将比例维持在easy:hard=100:1**。毕竟线上召回时，库里绝大多数的物料是与用户八杆子打不着的easy negative，保证easy negative的数量优势，才能hold住模型的及格线。



正负样本比例差距过大：只要符合线上真实分布就可以。





