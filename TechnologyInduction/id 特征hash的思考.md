ID类特征为什么要哈希

- 上述编码方式生成的Vector，其长度为总特征数，接入全连接层存在参数爆炸的问题。
- 通常一个样本对应的特征数量有限，远远小于总特征数。特征稀疏，每次更新只有极极极少数权重更新->网络收敛非常慢，有限样本情况下训练不充分。
- 上述编码方式，特征之间的关系始终保持独立，“非此即彼”，无法挖掘特征间的相似性。

热门商品id可以onthot，剩下的hash。



id类特征embedding后提供了什么信息（或能力）？*

不少同学最初的疑问是，这种空白或随机初始化的embedding到底提供了什么信息？事实上，它什么也没提供，只是让模型能够区分不同的id分类，为模型学习每个实体的信息提供了空间。

如果实体的已有特征已经能够提供模型需要的所有信息，那么可学习的embedding就没有太多效果。但不少领域我们对实体的了解的信息太少，但数据量较为充足，这时候可以给模型一个“存储空间，让其可以从数据中学习每个实体的特性。这在我见过的一些符合这类情况的问题下很明显。

另外id和其他特征组合后，会产生一些意义。比如刷单的人，通过id区别开来后，可以把本来他一个人提高的某个商品CTR权重，转移到这个刷单人的id特征上去。



但是一定要通过正则的方法来限制以使得id类特征不过拟合。



电商场景的user id embedding用处不大，但是也要作为特征是端到端训练

但在用户兴趣变化不大的场景下，比如食物推荐，user id embedding用处很大





https://www.zhihu.com/question/264165760/answer/1649994007



1. 一般而言这类技术是为了解决两个问题：一是将categorical的特征编码为模型可理解的格式，二是尽可能在保留有效信息的基础上降低训练和预测过程的时间复杂度。
2. 以上两个问题中，一是基础问题。One-Hot Serializing就可以达到这个效果，例如将训练样本中出现过的的每个deviceid按顺序递增编号（比如deviceid@xxx:1 -> feature@10000:1）。缺点是：1）这个映射表需要传递给引擎端，在prediction的时候照着再查一遍，数据量大且数据结构设计不好的时候很费时间；2）有些频次很低的特征置信度不高，单独出现对模型无益（甚至over-fitting）。这时候可以使用按频次截断技术进行降维。比如微软在deep crossing中提到的特征工程方法：只保留曝光最多的10k个campaign id编码为0-9999，其余的id全部编码为10000，但辅以poCTR等连续特征作为辅助。事实上这是一种手工的Hashing方法。
3. 自动Hashing方法的好处是：只要训练和预测时使用的hashing方法一致，对同一个特征的编码结果即一致，因此引擎预测或提供数据dump的时候无需查找编码表。所以其最大优点在于**数据一致性**和**速度提升**，这在极大规模特征和在线学习中至关重要。
4. 我们说的Hashing算法一般而言均特意设计为低碰撞率。因此一般hashing算法本身不会大幅降低特征维度，自然也不会大幅损失特征信息。**真正可能存在问题的是hashing之后的降维过程。**一个非常常见的陷阱是string哈希到int64后取模m，试图将特征压缩至m维one-hot空间。在这种情况下，对于不知情的随机hashing过程，不同特征的碰撞率为1/m。举个例子，对于“性别”特征，将male哈希为一个int64，female哈希为另一个int64，很难发生碰撞；但如果你试图使用mod2将其压缩，如果你的算法哈希出的这两个int64奇偶性相同，则会导致特征失效。在你有很多feature需要哈希而又不清楚hashing算法细节的情况下，这在概率意义上是很容易发生的。

因此我们会更倾向于所谓的embedding算法，例如将70万维的userid通过weight embedding到32维的**连续值**特征上（不同于传统hashing的低维离散值特征）。这意味着训练过程更加复杂（有更多的weight需要optimize）；但对于预测过程，其特征性能十分良好且时间复杂度得以降低。同时，由于连续值特征空间的表达能力大幅高于离散值特征空间，整个模型的表达能力并不会明显下降，也基本不会发生离散hashing的碰撞问题。（当然，如果是FM这类倾向于接受离散值的模型，手工serializing+精心设计的hashing是较好的选择。）







https://mp.weixin.qq.com/s/mOTfkA_BIg0tx0p1S4hT1Q

如何解决Hash冲突：

A.减少Hash冲突：1、增加映射后的维度，但是也不能加太多，不然失去hash映射的意义。2、增加哈希函数。

B.看冲突的特征：1、大部分小信息量特征之间碰撞无所谓。2、大信息量特征和小信息量特征碰撞，设计一个可训练重要性权重矩阵，对小信息量特征起特征正则的作用。3、大信息量特征之间碰撞，只能想办法减少哈希碰撞。



特征哈希的本质：

大佬1:相近特征分一个桶里

大佬2:把稀疏特征进行低维聚类，但是hash本身没法直接做到聚类效果



如何利用好长尾的离散特征？

特征hash，并把hash table size调小一些。

0初始化：避免随机值的影响

加正则可以让低频的更新幅度减少

频次过滤是设置准入门槛：不到一定次数不更新

特征聚类





DNN网络结构是超参数，有什么调参经验？

1、如果特征工程做得比较充分，原始特征又有冗余，正三角形。

2、相反用倒三角形，相当于矩阵分解，抽取隐向量

3、网络越深，特征抽象越高