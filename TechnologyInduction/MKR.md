>

采用多任务学习MKR的框架，可以有如下优势：

- 两者的可用信息可以互补；

- 知识图谱特征学习任务可以帮助推荐系统摆脱局部极小值；

- 知识图谱特征学习任务可以防止推荐系统过拟合；

- 知识图谱特征学习任务可以提高推荐系统的泛化能力。

  采用**交替训练**的方式：固定推荐系统模块的参数，训练知识图谱特征学习模块的参数；然后固定知识图谱特征学习模块的参数，训练推荐系统模块的参数。



从实际运用和时间开销上来说，**交替学习是介于依次学习和联合学习中间的**：训练好的知识图谱特征学习模块可以在下一次训练的时候继续使用（不像联合学习需要从零开始），但是依然要参与到训练过程中来（不像依次学习中可以直接使用实体向量）。



应该注意的是，交叉和压缩单元应该只存在于MKR的低层，如图a所示。 这是因为：

（1）在深层体系结构中，特征通常沿着网络从一般转换为特定，并且随着任务差异性的增加，特征可转移性在较高层中显着下降。因此，共享高层可能会带来负面转移，特别是对于MKR中的异构任务。
（2）在MKR的高级层中，物品特征与用户特征混合，实体特征与关系特征混合。 混合特征不适合共享，因为它们没有明确的关联。

虽然跨域推荐和转移学习对目标域有单一的目标，但它们的损失函数仍然包含测量源域内数据分布或两个域之间相似性的约束条件。在我们提出的MKR中，KGE任务明确地作为约束条件，为推荐系统提供正则化。

本文的主要贡献为通过多任务学习的方式对推荐问题进行建模: 相比于跨域（cross domain）和迁移学习（transfer learning），我们发现任务间的相似性不仅有助于提升推荐系统的效果,而且有助于增强图知识的向量表征特性。本文的理论分析和实验结果给出证明。



MKR是一个深入的端到端的框架，最初的用户向量u和物品向量v用one-hot来表示

